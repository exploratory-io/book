const template =
`# <%= model %>を使った<%= target %>と選択された予測変数の関係の分析

目的変数である<%= target %>と選択された予測変数との関係を調べるためにXGBoostを使った予測モデルが作られました。

# サマリ

## 予測精度

{{summary}}

**<%= target %>のばらつきの説明能力**

<% if (r_square > 0.8) { %>
R2乗は<%= r_square_pct %>% (<%= r_square %>)と非常に高く、これは選択された予測変数で<%= target %>のばらつきのほとんどを説明できることを示しています。
<% } else if (r_square > 0.6) { %>
R2乗は<%= r_square_pct %>% (<%= r_square %>)と高く、これは選択された予測変数で<%= target %>のばらつきをうまく説明できることを示しています。
<% } else if (r_square > 0.4) { %>
R2乗は<%= r_square_pct %>% (<%= r_square %>)と中程度ですが、選択された予測変数で<%= target %>のばらつきをそれなりに説明できることを示しています。
<% } else if (r_square > 0.2) { %>
R2乗は<%= r_square_pct %>% (<%= r_square %>)と低いですが、選択された予測変数で<%= target %>のばらつきのある程度の傾向は捉えているが、重要な予測変数が抜けている可能性があります。
<% } else { %>
R2乗は<%= r_square_pct %>% (<%= r_square %>)と非常に低いため、選択された予測変数では<%= target %>のばらつきはあまり説明できていません。
<% } %>

**予測精度**

RMSE（平均二乗誤差のルート）は<%= rmse %>で、これはこの予測モデルを使って元データを予測したときの予測値と実際の値の間の平均的な誤差を示しています。

## 選択された予測変数と<%= target %>の関係の強さ

選択された予測変数と<%= target %>の関係の強さを相対的に表したのが以下のチャートです。

{{variable_importance}}

## それぞれの予測変数と<%= target %>の関係

それぞれの予測変数の値が変わると、<%= target %>の値がどのように変わるかを表したのが以下のチャートです。変数は<%= target %>との関係が最も強いものから弱い方への順番で並んでいます。

{{variable_effect_numeric}}

# 次のステップ

* 変数選択の最適化：変数重要度が低い予測変数を除外して、モデルをシンプルにすることを検討してください。これにより、モデルの解釈がしやすくなり、過剰適合のリスクも減少します。
* グループ別分析：グループごとに別々のモデルを作成することで、それぞれのグループ内での<%= target %>の決定要因をより詳細に理解できるかもしれません。その場合は、「繰り返し」にグループとなる変数を選択し、実行し直すことができます。
* 外れ値の確認：予測精度に影響を与える可能性のある外れ値がないか確認し、必要に応じて対処することで、モデルの信頼性が向上する可能性があります。
* モデルの評価：このモデルの予測性能をより厳密に評価するために、トレーニングデータとテストデータに分けて検証することを検討してください。
* パラメータの調整：XGBoostのパラメータ（木の深さ、学習率、学習回数など）を調整することで、モデルの性能をさらに向上させることができるかもしれません。

# 補足情報

## トレーニングデータに対する予測

元のデータに対して、作成された予測モデルを使って予測した結果が以下の表となります。

{start_lazy_show_hide}
### チャート
{{data}}
{end_lazy_show_hide}

## 実測値と予測値の関係

予測した結果、元の実測値と予測値にはズレがありますが、それらの関係を散布図を使って可視化したのが以下のチャートです。それぞれの点はそれぞれの行を表しています。

{start_lazy_show_hide}
### チャート
{{actual_predicted}}
{end_lazy_show_hide}

## 学習回数と精度の向上

モデルの学習過程を表したのが以下のチャートです。横軸は学習回数、縦軸はRMSE（予測誤差）を表しています。
XGBoostは学習回数が増えるごとにRMSEが減少していき初期の学習ではRMSEが急速に減少し、その後は緩やかになっていく傾向があります。
学習曲線が平坦化してきた場合は、最適な学習回数に近づいている可能性があります。逆に、まだ誤差が減少し続けている場合は、さらに学習回数を増やすことで性能向上が見込める可能性があります。

{start_lazy_show_hide}
### チャート
{{learning_curve_numeric}}
{end_lazy_show_hide}
`;

module.exports = template;
