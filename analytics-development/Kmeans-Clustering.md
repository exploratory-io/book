const template = `


データの各行を観察対象として、選択された変数を元に「K-Means クラスタリング」を使ってデータを<%= cluster_number %>つのクラスター（グループ）に分類しました。クラスターの数は<%= cluster_number %>ですが、[「設定」](//analytics/settings)より変更可能です。

# 各クラスターの特徴

<!-- AI_SUMMARY -->

## サマリ情報

各クラスターのデータ量、そして分類に使われた変数に対する平均的なスコア（標準化された値）を以下の表にまとめています。

{{summary}}

* Excluded Rows（除外された行） -  1つでも変数に欠損値（NA）がある行は事前にデータから除外されます。Excluded Rowsの割合が高い場合、利用する変数を見直すか、欠損値を補完してください。

## レーダーチャートによる可視化

分類に使われた変数に対する平均的なスコア（標準化された値）を元にそれぞれのクラスターの特徴を解釈することができます。

{{radar}}

それぞれの軸の値はそれぞれの変数の元の値を標準化したものです。平均値は0で、正の値は平均よりも高いことを意味し、負の値は平均よりも低いことを意味します。


## 箱ひげ図による可視化

クラスターごとに、それぞれの変数における値のばらつきを箱ひげ図を使って可視化したものが以下です。

{{boxplot}}

Y軸の値はそれぞれの変数の元の値を標準化したものです。平均値は0で、正の値は平均よりも高いことを意味し、負の値は平均よりも低いことを意味します。

## 散布図による可視化

クラスターと変数の関係、そして変数間の関係を散布図を使って可視化したものです。これは、「バイプロット」と呼ばれるチャートです。

{{biplot}}

元のデータは<%= variables.length %>個の変数によって表されますが、全てのデータのばらつきを２次元（X軸とY軸）空間で可視化することができません。そこで、できる限り元のデータの持つ情報量（ばらつき）を失わないように、元のデータのばらつきを仮想的に作られた2つの変数（X軸、Y軸）によって可視化しようとしたものがこのチャートです。X軸は元データのばらつきの<%= ratio_of_variance_of_p1_pct %>%を表し、Y軸は<%= ratio_of_variance_of_p2_pct %>%を表しています。

それぞれの変数は中心点（0, 0）から外側に向かって伸びるグレーの線によって表されています。線が同じ方向に向かっている変数は相関が強い変数同士だと言えます。

それぞれのクラスターは、どの線に近いかによってその特徴を解釈することができます。外に向かって伸びる線の先に近ければ、その変数のスコアが高いクラスターだと言えます。逆に、0を挟んで逆側にある場合は、その変数のスコアが低いクラスターだと言えます。

<% if (has_category) { %>

# カテゴリーとの関係

カテゴリーとして選択された'<%= category %>'列とクラスターの関係をバーチャートを使って可視化したものが以下です。

{{category_ratio}}

<% } %>


# データ

クラスターの分類結果を元に、元のデータのそれぞれの行にクラスターのIDを割り当てました。

{{data}}


<% if (elbow_method_mode) { %>

# 最適なクラスター数

K-Means クラスタリングで「最適なクラスタ数（K）」を選ぶための視覚的な手法の1つとしてエルボー法が使えます。

仕組み：

1. クラスター数を1から10まで変化させながらK-Means クラスタリングを実行。
2. 各クラスター数に対してクラスター内のデータのばらつきの総和（クラスター内平方和）を計算。
3. クラスター数を横軸、クラスター内のばらつきを縦軸にラインチャートで可視化。
4. グラフの形が急激に減少したあとで緩やかになる「ひじ（Elbow）」となるあたりが最適なクラスター数。

必要ない場合は、[「設定」](//analytics/settings)より無効にできます。また最大クラスター数も変更可能です。

エルボー法についての詳細は、[こちらのノート](https://exploratory.io/note/exploratory/K-Means-QRV2jAz0#エルボーメソッドによるクラスター数の決定)をご参照ください。

## エルボーカーブ

エルボー法を使って、各クラスター数のクラスター内平方和をエルボーカーブと言われる線を使って可視化したのが以下のチャートです。

{{elbow}}

クラスタ数が少ないとクラスター内のデータのばらつきは大きいため、ある地点まではクラスター数を増やすとばらつきは大きく減少します。しかし、ある地点を過ぎると、ばらつきの減少量は徐々に小さくなります。この「急減から緩やかになる境目（ひじ（Elbow）」が「最適なクラスター数」としての目安です。ただ、ある数のクラスターに分けたあと、それぞれのクラスターの特徴が理解しやすく、有用であるかといった点も現実的には重要となるため、あくまでも参考程度にご使用ください。

## ばらつきの減少度

エルボーカーブのチャートでは、どこが「ひじ（Elbow）」となるのか視覚的にはっきりしない場合があります。そのような場合は、エルボーカーブの線の下降度（クラスター内のデータのばらつきの減少度）の数値を比べ、値が一定になる辺りが「最適なクラスター数」としての目安として参考にできます。

{{elbow_diff}}

<% } %>

# 補足情報

## 次のステップ

* クラスターに分けたい観察対象が現在の行に対応していない場合、K-Means クラスタリング用のデータを作成する必要があります。詳細は、[こちらのノート](https://exploratory.io/note/exploratory/K-Means-sfp4Syw0)をご参照ください。


`;

module.exports = template;
