const template = `

# モデルの指標

<% if (!test_mode) { %>
モデルの予測精度や有意性に関する様々な指標を以下の表にまとめています。
<% } else { %>
モデルの予測精度や有意性に関する様々な指標を、以下の表にまとめています。現在テストモードであるため、予測精度に対してはトレーニングデータとテストデータ両方に対しての指標を表示しています。
<% } %>

{{summary}}

## 予測精度

### AUC

ロジカル型（TRUE/FALSE）を予測するモデルの予測精度の評価には一般的にAUCがよく使われます。値は0.5から1の間で、0.5はランダムな予測（コイン投げと同等）、1はTRUEとFALSEのデータを完全に分類できることを意味します。

<% if (auc > 0.9) { %>
AUCは<%= auc_pct %>% (<%= auc %>)と非常に高く、このモデルは<%= target %>のTRUEとFALSEのデータを非常にうまく分類できることを示しています。
<% } else if (auc > 0.8) { %>
AUCは<%= auc_pct %>% (<%= auc %>)と高く、このモデルは<%= target %>のTRUEとFALSEのデータをうまく分類できることを示しています。
<% } else if (auc > 0.6) { %>
AUCは<%= auc_pct %>% (<%= auc %>)と中程度で、このモデルは<%= target %>のTRUEとFALSEのデータをある程度うまく分類できることを示しています。
<% } else { %>
AUCは<%= auc_pct %>% (<%= auc %>)と低く、このモデルは<%= target %>のTRUEとFALSEをあまりうまく分類できないことを示しています。
<% } %>

### 正解率、F値

* 正解率、誤分類率、F1スコア、適合率、再現率はTRUE/FALSEの境界値の設定により影響されます。現在の境界値は<%= true_false_criteria %>に設定されていますが、これは[「設定」](//analytics/settings)より変更可能です。

{start_show_hide}
## モデルの指標の詳細
* AUC
  * AUC（Area Under the Curve）はROC曲線の下の面積で、モデルの分類性能を総合的に評価する指標です。
  * 値は0.5から1の間で、0.5はランダムな予測（コイン投げと同等）、1はTRUEとFALSEをうまく切り分けられている完璧な分類を意味します。
  * 一般的に0.7以上で許容可能、0.8以上で良好、0.9以上で非常に優れた分類性能と解釈されます。

* F1スコア
  * F1スコアは適合率と再現率の調和平均で、両方のバランスを考慮した分類性能の指標です。
  * 値は0から1の間で、1に近いほど精度と網羅性のバランスが取れた優れた分類器を意味します。
  * 元データのTRUEとFALSEの割合に大きな差がある場合に特に有用です。

* 正解率
  * 正解率（Accuracy）は全予測中の正しい予測の割合を示します。
  * 値は0から1の間で、1に近いほど多くの事例を正しく分類できることを意味します。
  * 元データのTRUEとFALSEの割合に大きな差がある場合、不均衡がある場合は誤解を招く可能性があります。

* 誤分類率
  * 誤分類率（Error Rate）は全予測中の誤った予測の割合で、1-正解率で計算されます。
  * 値は0から1の間で、0に近いほど誤分類が少ないことを意味します。
  * 正解率と同様に、元データのTRUEとFALSEの割合に大きな差がある場合は解釈に注意が必要です。

* 適合率 (Precision)
  * 適合率は「TRUEと予測したもののうち、実際にTRUEだった割合」を示します。
  * 値は0から1の間で、1に近いほど「TRUEと予測したものが実際にTRUEである精度」が高いことを意味します。
  * 偽陽性（実際はFALSEなのにTRUEと予測）を最小化したい場合に重視される指標です。

* 再現率 (Recall)
  * 再現率は「実際にTRUEであるもののうち、TRUEと正しく予測できた割合」を示します。
  * 値は0から1の間で、1に近いほど「実際のTRUEのケースを見逃さない能力」が高いことを意味します。
  * 偽陰性（実際はTRUEなのにFALSEと予測）を最小化したい場合に重視される指標です。

* 行数 (TRUE)
  * 目的変数がTRUEのデータの行数を示します。
  * 極端にTRUEが少ない場合、モデルの学習や評価に偏りが生じる可能性があります。

* 行数 (FALSE)
  * 目的変数がFALSEのデータの行数を示します。
  * 極端にFALSEが少ない場合、モデルの学習や評価に偏りが生じる可能性があります。

* 行数
  * 行数は分析に使用したデータの総数（サンプルサイズ）です。
  * データ数が多いほど、モデルの信頼性が高まります。

* 対数尤度
  * 対数尤度はモデルがデータにどれだけ適合しているかを数値化したものです。
  * 通常は負の値を取り、0に近いほどモデルの適合度が高いことを示します。
  * 単独では解釈が難しく、モデル比較のためのAICやBICの計算に使用されます。
{end_show_hide}

# 変数間の関係

<% if (predictorColumns.length > 1) { %>
## 説明変数の重要度

<%= target %>を予測するためにどの説明変数が相対的により重要なのかを表したのが以下のチャートです。

<% if (with_boruta) { %>
  {{variable_importance_boxplot}}
<% } else { %>
  {{variable_importance}}
<% } %>


## 説明変数の影響度

それぞれの説明変数の値が変わると、<%= target %>の確率がどのように変わるかを表したのが以下のチャートです。

{{variable_effect}}

説明変数が数値型の場合は、X軸の値が変化した時に予測値がどのように変化していくのかをラインチャートで可視化しています。説明変数が文字列型、ロジカル型の場合はその値での予測値を点として表現しています。

<% if (predictorColumns.length > 1) { %>

_説明変数は上記の「説明変数の重要度」にある重要度の高い順番で並んでいます。_
<% } %>

# 補足情報

<% if (!test_mode) { %>
## トレーニングデータに対する予測

トレーニングデータに対して、作成された予測モデルを使って予測した結果が以下の表となります。
<% } else { %>
## トレーニング・テストデータに対する予測

トレーニングデータとテストデータに対して、作成された予測モデルを使って予測した結果が以下の表となります。
<% } %>

{start_lazy_show_hide}
### 予測結果
{{data}}
{end_lazy_show_hide}

## 予測マトリックス（混同行列）

<% if (!test_mode) { %>
このモデルは<%= mode %>データのそれぞれの行に対してTRUEまたはFALSEと予測したわけですが、そのうちのどれだけが実測値においてTRUEまたはFALSEだったのかを対応表としてまとめたのが以下の表です。数値はそれぞれの組み合わせが全データに占める割合（％）です。

<% } else { %>
このモデルは<%= mode %>データのそれぞれの行に対してTRUEまたはFALSEと予測したわけですが、そのうちのどれだけが実測値においてTRUEまたはFALSEだったのかを対応表としてまとめたのが以下の表です。数値はそれぞれの組み合わせが全データに占める割合（％）です。現在テストモードであるため、トレーニングデータ、テストデータそれぞれに対する結果を表示しています。

<% } %>

{{confusion_matrix}}

## 予測確率の分布

このモデルは<%= mode %>データのそれぞれの行に対して<%= target %>の確率（0～1の間の値）を予測したわけですが、この確率の値の分布を実測値がTRUE（青）、FALSE（オレンジ）のグループに分けて可視化したのが以下のチャートです。

{{probability_distribution}}

<% if (test_mode) { %>
* 現在テストモードであるため、テストデータに対して予測した確率の分布となります。
<% } %>
* 実測値がTRUEのグループ（青線）は右側（1に近い確率）に、実測値がFALSEのグループ（オレンジ線）は左側（0に近い確率）に偏っているほど、モデルの予測精度が高いと言えます。
* 縦の点線は、TRUEとFALSEを分類するために現在設定されている確率の境界値を示しています。デフォルトは50％（0.5）ですが、[「設定」](//analytics/settings)より変更可能です。
* この分布からモデルの分類性能や、最適なTRUE/FALSEの境界値の調整を視覚的に確認できます。

## ROC曲線

<% if (!test_mode) { %>
モデルの分類性能を様々なTRUE/FALSEの境界値で評価するROC曲線が以下のチャートです。Y軸は真陽性率（感度）、X軸は偽陽性率（1-特異度）を表しています。青い線は今回のモデルのROC曲線で、対角線の灰色の点線はランダムな予測（AUC: 0.5）を意味します。ROC曲線は左上に膨らむほど予測精度が高く、対角線に近いほど予測精度が低いことを示します。
<% } else { %>
モデルの分類性能を様々なTRUE/FALSEの境界値で評価するROC曲線が以下のチャートです。Y軸は真陽性率（感度）、X軸は偽陽性率（1-特異度）を表しています。青い線は今回のモデルのテストデータに対するROC曲線、オレンジの線はトレーニングデータに対するROC曲線です。対角線の灰色の点線はランダムな予測（AUC: 0.5）を意味します。ROC曲線は左上に膨らむほど予測精度が高く、対角線に近いほど予測精度が低いことを示します。
<% } %>

{{roc_curve}}

# 次のステップ

* 変数選択の最適化：変数重要度が低い予測変数を除外して、モデルをシンプルにすることを検討してください。これにより、モデルの解釈がしやすくなり、過剰適合のリスクも減少します。
* グループ別分析：グループごとに別々のモデルを作成することで、それぞれのグループ内での<%= target %>の決定要因をより詳細に理解できるかもしれません。その場合は、「繰り返し」にグループとなる変数を選択し、実行し直すことができます。
* 外れ値の確認：予測精度に影響を与える可能性のある外れ値がないか確認し、必要に応じて対処することで、モデルの信頼性が向上する可能性があります。
* TRUE/FALSEの境界値の調整：予測のTRUE/FALSEの境界値を調整することで、適合率と再現率のバランスを変えることができます。TRUE/FALSEの境界値を変更したい場合は、設定から変更が可能です。
<% if (!test_mode) { %>
* モデルの評価：このモデルの予測性能をより厳密に評価するために、トレーニングデータとテストデータに分けて検証することができます。その場合は、[「設定」](//analytics/settings)より「検証」セクションの下の「テストモード」をTRUEに設定し、実行し直してください。
<% } %>

`;

module.exports = template;
